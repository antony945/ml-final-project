cartpole:
  env_id: CartPole-v1
  replay_memory_size: 100_000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.9995
  epsilon_min: 0.05

frozenlake:
  env_args:
    id: FrozenLake-v1
    map_name: 8x8
    is_slippery: True
  replay_memory_size: 100_000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.9995
  epsilon_min: 0.05
  n_episodes: 100_000
  learning_rate: 0.001

taxi:
  env_args:
    id: Taxi-v3
  replay_memory_size: 100_000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.9995
  epsilon_min: 0.05
  n_episodes: 100_000
  learning_rate: 0.001

lunarlander:
  env_args:
    id: LunarLander-v3
    continuous: False
  replay_memory_size: 100_000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.9995
  epsilon_min: 0.05
  n_episodes: 5_000
  learning_rate: 0.001

mountaincar:
  env_args:
    id: MountainCar-v0
  replay_memory_size: 100_000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.9995
  epsilon_min: 0.05
  n_episodes: 5_000
  learning_rate: 0.9
  max_episode_steps: 1000

tetris:
  env_args:
    id: tetris_gymnasium/Tetris
  replay_memory_size: 100_000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.9995
  epsilon_min: 0.05
  n_episodes: 100_000
  learning_rate: 0.001

flappybird_q:
  dqn: False
  env_args:
    id: FlappyBird-v0
    use_lidar: False
  # Both approaches
  # seed: 32
  epsilon_init: 1
  epsilon_decay: 0.99995
  epsilon_min: 0.05         # 0.05 seems to perform better than 0.10
  learning_rate: 0.001
  discount_factor: 0.95
  # n_episodes: 500_000
  # stop_on_reward: 100
  enable_ER: false           # enable Experience Replay (ER)
  enable_PER: false          # enable Prioritized Experience Replay (PER) (overrides enable_ER)
  alpha_PER: 0.6            # alpha for PER
  beta_init_PER: 0.4        # beta init for PER
  beta_final_PER: 1.0       # beta final for PER
  mini_batch_size: 64       # 64 seems to perform better than 32 for DQN (but needs to try)
  min_memory_size: 64
  max_memory_size: 131_072  # to use power of 2, 2^17
  lazy_update: false        # true seems to perform better than false for DQN
  # Q-Table specific
  divisions: 15

flappybird_dqn:
  dqn: False
  env_args:
    id: FlappyBird-v0
    use_lidar: False
  # Both approaches
  # seed: 32
  epsilon_init: 1
  epsilon_decay: 0.99995
  epsilon_min: 0.01          # 0.01 performs way better than 0.03 and 0.05 (for Q-Table)
  learning_rate: 0.0001      # 0.0001 with 1kk performs better than 0.001
  discount_factor: 0.95
  n_episodes: 1_000_000
  # stop_on_reward: 100
  enable_ER: true           # enable Experience Replay (ER)
  enable_PER: true         # enable Prioritized Experience Replay (PER) (overrides enable_ER)
  alpha_PER: 0.6            # alpha for PER
  beta_init_PER: 0.4        # beta init for PER
  beta_final_PER: 1.0       # beta final for PER
  mini_batch_size: 32       # 64 seems to perform better than 32 for DQN (but needs to try)
  min_memory_size: 32
  max_memory_size: 131_072  # to use power of 2, 2^17
  lazy_update: true         # true seems to perform better than false for DQN
  # DQN specific
  use_target_dqn: true
  network_sync_rate: 10
  fc1_nodes: 128
  # Q-Table specific
  divisions: 10 # something between 8 and 12, 10 perfect value, anything <=5 or >= 15 is bad